{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект курса ML1 Анализ веб-документов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Приветствие"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здравствуйте, уважаемые студенты! Рады представить Вам проект по анализу данных. Этот проект поможет Вам на практике попробовать не только те техники, которые мы рассказываем в курсе, но и, возможно, изобрести что-то новое, уникальное. Хотелось, чтобы этот ноутбук не только рассказывал про проект, но и дал Вам некоторые ориентиры, как в принципе стоит подходить к решению незнакомой задачи анализа данных. Надеемся, что наши советы окажутся Вам интересны и помогут скорее стать большим профессионалом. \n",
    "\n",
    "Удачи! Будет интересно!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Обзор проекта"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во множестве прикладных задач возникает необходимость разбить веб-страницы на какие-то определенные группы, где в каждой группе страницы будут очень похожи по смыслу. Например, представим, что Вы владелец сервиса интернет рекламы. Вашим клиентам хочется, чтобы их услуги рекламировались не на каком-то определенном сайте, а на всех сайтах их тематики. То есть Вам нужна какая-то тематическая разметка сайтов в интернете по множеству тематик, и клиент сможет выбрать любую, какая ему больше нравится. Как же такую разметку сделать?\n",
    "\n",
    "Самый простой способ - разметить людьми множество сайтов в интернете на множество тем, настроить на этой выборке многоклассовый классификатор и сделать предсказанием на всем оставшемся \"интернете\". Однако, в таком подходе возникает несколько проблем. Во-первых, непонятно откуда брать само множество тем, на которое размечать. Во-вторых, даже если множество тем зафиксировано, не очень понятно, откуда для каждой темы брать примеры, чтобы добавить их в обучающее множество.\n",
    "\n",
    "В данном задании мы предлагаем Вам попробовать другое решение. Пусть тематику задают сами данные! Разделим наши веб-страницы на множество групп, например, просто по словам в веб-страницах. В такой группе буду как документы об одном и том же, так и \"аномалии\", которые имеют схожие слова, но не соответствуют документам основной тематики. Например, в такой группе может содержаться подмножество веб-страниц про \"ремонт пластиковых окон\" и аномалии вроде \"пластиковые игрушки\", \"ремонт квартир\" и так далее. Нам останется только выделить подмножество документов одной темы, то есть все документы, которые про \"ремонт пластовых окон\" и убрать все аномалии. Затем подмножество как-то проименуем, чтобы показать клиенту, но этим Вы уже займетесь, когда будете продавать Вашу систему :)\n",
    "\n",
    "В задании Вам предлагается работать с 28026 веб-страницами, которые уже скачаны и лежат в архиве content.tar.gz. Эти страницы разбиты по группам, каждая группа около 100 страниц. Каждая группа соответствует какой-то определенной теме, которая Вам неизвестна. Обучающее множество состоит из 129 групп. В обучающих группах ручной разметкой было проставлено, соответствует ли данный документ теме группы (target = 1) или это аномалия (target = 0). Тестовое множество состоит из 180 групп. Вам необходимо проставить для них target. Важно отметить, что обучающие и тестовые группы не пересекаются. Гарантируется, что в каждой группе есть подмножество документов из ее темы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее будем называть веб-страницы, которые соотвествуют теме группы, \"настоящими\", а которые не соотвествуют - \"выбросами\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Любой аналитик данных первым делом должен посмотреть на сами данные. Возьмем две группы из обучающего множества и посмотрим на две настоящих страницы и на выброс. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Группа 3. Настоящий объект\n",
    "![title](images/g3_true_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Группа 3, Настоящий объект\n",
    "![image.png](images/g3_true_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Группа 3. Выброс\n",
    "![image.png](images/g3_outlier.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется, что довольно просто. А посложнее?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Группа 1. Настоящий объект\n",
    "![image.png](images/g1_true_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Группа 1. Настоящий объект. \n",
    "![image.png](images/g1_true_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Группа 1. Выброс\n",
    "![image.png](images/g1_outlier.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача выглядит сложно, но человек с ней справляется, а значит есть шанс, что машина тоже справится. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Обзор данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перечислим, какие данные мы дали для решения конкурса. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Контент веб-страниц\n",
    "\n",
    "Лежит в архиве content.tar.gz. В директориии 28026  веб-страниц, с которыми мы будем работать в конкурсе. Парсить их можно с помощью волшебной библиотеки BeautifulSoup. Рекомендуется почитать в интернете как строится html, чтобы понимать, какие данные можно извлечь из веб-страницы и почитать мануал по этой билиотеке https://www.crummy.com/software/BeautifulSoup/bs4/doc/ Во всех файлах .dat первой строчкой указан урл веб-страницы, на случай, если Вы захотите скачать веб-страницу самостоятельно или использовать урл в качестве признаков. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'content/1.dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bf9ccfa1caf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1.dat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sphere-py37/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, encoding, errors, buffering)\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;31m# Force opening of the file in binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'content/1.dat'"
     ]
    }
   ],
   "source": [
    "# Пример, как можно достать урл и заголовок веб-страницы\n",
    "from bs4 import BeautifulSoup\n",
    "import codecs\n",
    "path = 'content/'\n",
    "filename = '1.dat'\n",
    "\n",
    "with codecs.open(path + filename, 'r', 'utf-8') as f:\n",
    "    text = BeautifulSoup(f, 'lxml').title.text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Заголовки веб-страниц\n",
    "Парсинг веб-страниц занимает довольно много времени (порядка секунды на страницу), поэтому для этой демонстрации и домашних работ мы извлекли из них заголовки и сохранили. Лежат в файле docs_titles.tsv.  Для решения проекта настоятельно рекомендуется не использовать этот файл, а извлечь данные самому, так как страница соотвествует множество полезной информации помимо заголовка. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "title_data = pd.read_csv('docs_titles.tsv', sep='\\t', encoding='utf-8', lineterminator='\\n')\n",
    "title_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пандас довольно криво работает с текстами, так что я рекомендую считывать этот файл \"руками\", чтобы корректно все обработать. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Соответствия веб-страниц группам. Группы обучения это train_groups.csv, группы предсказания test_groups.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_groups = pd.read_csv('train_groups.csv')\n",
    "print (train_groups.shape)\n",
    "train_groups.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "doc_id - Уникальный идентификатор веб-страницы\n",
    "\n",
    "group_id - Уникальный идентификатор группы веб-страниц, среди которой нужно найти подмножество одной тематики\n",
    "\n",
    "pair_id - Уникальный идентификатор пары (group_id, doc_id)\n",
    "\n",
    "target - Значение, которое нужно предсказать. 1 - страница настоящая, 0 - выброс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_groups = pd.read_csv('test_groups.csv')\n",
    "print (test_groups.shape)\n",
    "test_groups.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важно отметить, обучающие и тестовые группы не пересекаются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть нужно предсказать, какие веб-страницы являются настоящими для тестовых групп, когда у Вас нет ни одного настоящего примера из этих групп. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Пример предсказания на тестовом множестве sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_subm = pd.read_csv('sample_submission.csv')\n",
    "print (sample_subm.shape)\n",
    "sample_subm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Пример предсказания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как же такую задачу решать? В  анализе данных нет правильного решения. Здесь никому не важен алгоритм которым Вы решали, данные которые Вы использовали. Важен финальный результат.\n",
    "Нам понравится любое решение, которое будет хорошо решать задачу :) Далее мы будем рассказывать, как нам кажется  стоит решать эту задачу, но Вы вольны делать ее, как Вам больше нравится. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При решении любой задачи анализа данныъ нужно определить, что является объектом, то есть для какой сущности мы хотим делать предсказания. В этой задаче объектом является пара (группа, веб-страница). Не имеет большого смысла говорить просто об объекте, без какой-либо привязки  к группе. Поэтому большинство признаков должно одновременно описывать отношение объекта с группой. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С этой точки зрения на наша задача превращается во что-то подобное:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/anomaly_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть в группе есть множество страниц, тесно друг с другом связанных, а есть страницы, которые не связаны  или слабо связаны с другими страницами в группе. Очевидно, первые -- настоящие, а вторые -- выбросы. Как задать величину \"связи\"?  Самый простой и очевидный способ -- число общих слов в заголовках!\n",
    "\n",
    "Сделаем датасет с признаками вида \"макс число общих слов с другими объектами из группы\", \"предмакс число общих слов с другими объектами из группы\" ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_to_title = {}\n",
    "with open('docs_titles.tsv') as f:\n",
    "    for num_line, line in enumerate(f):\n",
    "        if num_line == 0:\n",
    "            continue\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "        else:\n",
    "            title = data[1]\n",
    "        doc_to_title[doc_id] = title\n",
    "print (len(doc_to_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перегоняем пандас в словарь, потому что я не люблю пандас :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = 'content/'\n",
    "\n",
    "train_data = pd.read_csv('train_groups.csv')\n",
    "traingroups_titledata = {}\n",
    "for i in range(len(train_data)):\n",
    "    new_doc = train_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    title = doc_to_title[doc_id]\n",
    "\n",
    "    if doc_group not in traingroups_titledata:\n",
    "        traingroups_titledata[doc_group] = []\n",
    "    traingroups_titledata[doc_group].append((doc_id, title, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем любую группу и посмотрим сколько общих слов с другими страницами из группы у настоящей веб-страницы и у выброса.  Номера объектов мы подсмотрели заранее. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_group_id = 3\n",
    "outlier_doc_num = 0\n",
    "true_doc_num = 2\n",
    "outlier_doc_words = set(traingroups_titledata[check_group_id][outlier_doc_num][1].split())\n",
    "true_doc_words = set(traingroups_titledata[check_group_id][true_doc_num][1].split())\n",
    "outlier_doc_intersect = []\n",
    "true_doc_intersect = []\n",
    "for i in range(len(traingroups_titledata[check_group_id])):\n",
    "    doc_words = set(traingroups_titledata[check_group_id][i][1].split())\n",
    "    if i != outlier_doc_num:\n",
    "        outlier_doc_intersect.append(len(outlier_doc_words.intersection(doc_words)))\n",
    "    if i != true_doc_num:\n",
    "        true_doc_intersect.append(len(true_doc_words.intersection(doc_words)))\n",
    "        \n",
    "print (max(outlier_doc_intersect))\n",
    "print (max(true_doc_intersect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У выброса максимум одно общее слово с другой веб страницей! Отрисуем теперь гистограмму для числа общих слов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.hist(outlier_doc_intersect)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(true_doc_intersect)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проведем то же самое исследование для тех же групп, используя pymorphy2, который поможет нам привести слова к нормальной форме и улучшить показатели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2 as pym2\n",
    "\n",
    "check_group_id = 3\n",
    "outlier_doc_num = 0\n",
    "true_doc_num = 2\n",
    "outlier_doc_words = set(traingroups_titledata[check_group_id][outlier_doc_num][1].split())\n",
    "true_doc_words = set(traingroups_titledata[check_group_id][true_doc_num][1].split())\n",
    "outlier_doc_intersect = []\n",
    "true_doc_intersect = []\n",
    "for i in range(len(traingroups_titledata[check_group_id])):\n",
    "    doc_words = set(traingroups_titledata[check_group_id][i][1].split())\n",
    "    if i != outlier_doc_num:\n",
    "        outlier_doc_intersect.append(len(outlier_doc_words.intersection(doc_words)))\n",
    "    if i != true_doc_num:\n",
    "        true_doc_intersect.append(len(true_doc_words.intersection(doc_words)))\n",
    "        \n",
    "print (max(outlier_doc_intersect))\n",
    "print (max(true_doc_intersect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.hist(outlier_doc_intersect)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(true_doc_intersect)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разница в гистрограмме огромная! Значит метод должен сработать.\n",
    "\n",
    "Теперь посчитаем общие слова для всех групп и всех веб-страниц. Как мы видели ранее, у аномалий максимум общих слов значительно меньше. Но чтобы подстраховаться от пары выбросов с одинаковыми словами в заголовке, возьмем не один максимум, а все топ-n значений. Например, как признаки для веб-страницы, возьмем значения топ-15  пересечений с другими страницами из группы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "import re\n",
    "import pymorphy2 as pym2\n",
    "\n",
    "# функция для проверки соответствия слова какой-либо информативной единице\n",
    "def pass_pattern(s):\n",
    "    x = re.findall(r'[a-zа-я0-9]*\\b', s)\n",
    "    if x:\n",
    "        return x[0] == s\n",
    "    return False\n",
    "\n",
    "y_train = []\n",
    "X_train = []\n",
    "groups_train = []\n",
    "words_list = {}\n",
    "\n",
    "#используем морфологический анализатор\n",
    "MAnalyzer = pym2.MorphAnalyzer()\n",
    "\n",
    "# множество словарь слов с их количеством в тектсе\n",
    "multi_map = {}\n",
    "\n",
    "# идем по всем группам и считаем сколько каких слов встречается во всех заголовках\n",
    "# данные слова должны быть приведены к нормальной форме\n",
    "for new_group in traingroups_titledata:\n",
    "    docs = traingroups_titledata[new_group]\n",
    "    for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "        words = set([MAnalyzer.normal_forms(x.lower().strip('.,?:!-()\";*\"'))[0]\n",
    "                     if pass_pattern(x.lower().strip('.,?:!()\";*\"')) else ''\n",
    "                     for x in title.strip().split()])\n",
    "        if '' in words:\n",
    "            words.remove('')\n",
    "        words_list[doc_id] = words.copy()\n",
    "        for x in words:\n",
    "            if not x in multi_map:\n",
    "                multi_map[x] = 1\n",
    "            else:\n",
    "                multi_map[x] += 1\n",
    "\n",
    "# выбираем количество наиболее часто встречающихся слов, которые не являются информативными единицами\n",
    "# выводим наиболее часто встречающиеся слова в выборке тайтлов\n",
    "MAX_SPEC_WORDS = 11\n",
    "popular_words_lst = list(multi_map.items())\n",
    "popular_words_lst.sort(key=lambda x: x[1], reverse=True)\n",
    "print(*popular_words_lst[:3 * MAX_SPEC_WORDS], sep='\\n')\n",
    "\n",
    "# добавляем еще несколько неинформативных единиц, которые не являются достаточно популярными, \n",
    "# чтобы попасть в топ-MAX_SPEC_WORDS\n",
    "spec_words = {'о', 'из'}\n",
    "for i in popular_words_lst[:MAX_SPEC_WORDS]:\n",
    "    spec_words.add(i[0])\n",
    "print(spec_words)\n",
    "\n",
    "print('popular -- ok')\n",
    "\n",
    "max_features = 50 # количество фичей для каждого тайтла в обучающей выборке\n",
    "cnt = 1           # номер текущей группы \n",
    "for new_group in traingroups_titledata:\n",
    "    docs = traingroups_titledata[new_group]\n",
    "    for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "        groups_train.append(new_group)\n",
    "        all_dist = []\n",
    "        # приводим слова к нормальной форме и помещаем их в сет\n",
    "        words = words_list[doc_id].copy()\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j, target_j = docs[j]\n",
    "            # приводим слова к нормальной форме и помещаем их в сет\n",
    "            words_j = words_list[doc_id_j].copy()\n",
    "            all_dist.append(len(words.intersection(words_j)))\n",
    "        # если количество фичей недостаточно большое, то добавляем фиктивные (запонляем их нулями) \n",
    "        if len(all_dist) < max_features:\n",
    "            all_dist.extend([0] * (max_features - len(all_dist)))\n",
    "        X_train.append(sorted(all_dist, reverse=True)[0:max_features])\n",
    "        y_train.append(target_id)\n",
    "    print(\"{} -- ok\".format(cnt))\n",
    "    cnt += 1\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "groups_train = np.array(groups_train)\n",
    "print (X_train.shape, y_train.shape, groups_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отрисуем классы в пространстве первых двух признаков. \n",
    "\n",
    "Видно, что выбросы в среднем  расположены слева снизу, но есть и несколько исключений из этого правила."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Задание:}$\n",
    "\n",
    "1) Разбейте выборку на обучение и валидацию (как это сделать правильно? не забудьте, как разбиты группы в обучении и тесте, валидация должна повторять эту логику)\n",
    "\n",
    "2) Обучите метод ближайшего соседа, проверьте  качество на валидации. Не забудьте использовать нормализацию признаков!\n",
    "\n",
    "3) Не забывайте, что метод ближайшего соседа не оптимизирует F1. Как по выходу метода ближайшего соседа сделать оптимальное предсказание с точки зрения метрики F1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем значение для регрессора, которое отличает класс 0 от класса 1, с помощью валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = normalize(X_train, norm='l2', axis=1)\n",
    "\n",
    "cnt = 1\n",
    "val = np.linspace(0, 1, 80)\n",
    "res = []\n",
    "index = 0\n",
    "for x in val: # x - пороговое значение для KNeighborsRegressor\n",
    "    cur_res = []\n",
    "    for _ in range(10):\n",
    "        # делим выборку на валидационную и тренировочную\n",
    "        X_subtrain, X_validation, y_subtrain, y_validation = train_test_split(X_train, y_train, test_size = 0.3)\n",
    "\n",
    "        #clf = KNeighborsRegressor(n_neighbors=35, algorithm='kd_tree')\n",
    "        clf = GradientBoostingClassifier()\n",
    "        clf.fit(X_subtrain, y_subtrain)\n",
    "\n",
    "        y_validation_predict = clf.predict(X_validation)\n",
    "        # решаем, к какому классу принадлежит объект в зависимости от значения регрессора\n",
    "        for i in np.where(y_validation_predict >= x):\n",
    "            y_validation_predict[i] = 1\n",
    "        for i in np.where(y_validation_predict < x):\n",
    "            y_validation_predict[i] = 0\n",
    "        # сохраняем значения f1 score\n",
    "        cur_res.append(f1_score(y_validation, y_validation_predict))\n",
    "    \n",
    "    print(\"{} -- ok\".format(cnt))\n",
    "    cnt += 1\n",
    "    # находим среднее значение\n",
    "    res.append(sum(cur_res) / len(cur_res))\n",
    "    if res[-1] > res[index]:\n",
    "        index = len(res) - 1\n",
    "\n",
    "plt.plot(val, res)\n",
    "print(res[index], val[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем значение количества ближайших соседей, для которого достигается наилучее значение скора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_n = []\n",
    "k_index = 0\n",
    "cnt = 0\n",
    "for k_nei in range(1, 51):\n",
    "    cur_res = []\n",
    "    for j in range(15):\n",
    "        X_subtrain, X_validation, y_subtrain, y_validation = train_test_split(X_train, y_train, test_size = 0.3)\n",
    "    \n",
    "        clf = KNeighborsRegressor(n_neighbors=k_nei, algorithm='kd_tree')\n",
    "        clf.fit(X_subtrain, y_subtrain)\n",
    "\n",
    "        y_validation_predict = clf.predict(X_validation)\n",
    "        for i in np.where(y_validation_predict >= val[index]):\n",
    "            y_validation_predict[i] = 1\n",
    "        for i in np.where(y_validation_predict < val[index]):\n",
    "            y_validation_predict[i] = 0\n",
    "\n",
    "        cur_res.append(f1_score(y_validation, y_validation_predict))\n",
    "    k_n.append(sum(cur_res) / len(cur_res))\n",
    "    if k_n[-1] > k_n[k_index]:\n",
    "        k_index = len(k_n) - 1\n",
    "    print(\"ok -- {}\".format(cnt))\n",
    "    cnt += 1\n",
    "\n",
    "plt.plot(range(1, 51), k_n)\n",
    "print(k_index + 1, k_n[k_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если успешно справились, сделайте предсказание на тестовом множестве и залейте Ваше решение на платформу kaggle.  Посмотрите результат, попробуйте его улучшить. Потом еще раз улучшить :)\n",
    "\n",
    "Вы готовы заниматься анализом данных!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_groups.csv')\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#просто копипастим и загоняем тестовую выборку в test_data так же кк train_data\n",
    "testgroups_titledata = {}\n",
    "for i in range(len(test_data)):\n",
    "    new_doc = test_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in testgroups_titledata:\n",
    "        testgroups_titledata[doc_group] = []\n",
    "    testgroups_titledata[doc_group].append((doc_id, title))\n",
    "\n",
    "def pass_pattern(s):\n",
    "    x = re.findall(r'[a-zа-я0-9]*\\b', s)\n",
    "    if x:\n",
    "        return x[0] == s\n",
    "    return False\n",
    "    \n",
    "# анализатор слов\n",
    "MAnalyzer = pym2.MorphAnalyzer()\n",
    "\n",
    "# множество словарь слов с их количеством в тектсе\n",
    "multi_map = {}\n",
    "\n",
    "words_list = {}\n",
    "\n",
    "for new_group in testgroups_titledata:\n",
    "    docs = testgroups_titledata[new_group]\n",
    "    for k, (doc_id, title) in enumerate(docs):\n",
    "        words = set([MAnalyzer.normal_forms(x.lower().strip('.,?:!-()\";*\"'))[0]\n",
    "                     if pass_pattern(x.lower().strip('.,?:!()\";*\"')) else ''\n",
    "                     for x in title.strip().split()])\n",
    "        if '' in words:\n",
    "            words.remove('')\n",
    "        words_list[doc_id] = words.copy()\n",
    "        for x in words:\n",
    "            if not x in multi_map:\n",
    "                multi_map[x] = 1\n",
    "            else:\n",
    "                multi_map[x] += 1\n",
    "\n",
    "# выбираем количество наиболее часто встречающихся слов, которые не являются информативными единицами\n",
    "# выводим наиболее часто встречающиеся слова в выборке тайтлов\n",
    "MAX_SPEC_WORDS = 11\n",
    "popular_words_lst = list(multi_map.items())\n",
    "popular_words_lst.sort(key=lambda x: x[1], reverse=True)\n",
    "print(*popular_words_lst[:3 * MAX_SPEC_WORDS], sep='\\n')\n",
    "\n",
    "# добавляем еще несколько неинформативных единиц, которые не являются достаточно популярными, \n",
    "# чтобы попасть в топ-MAX_SPEC_WORDS\n",
    "spec_words = {'от', 'из', 'к', 'за'}\n",
    "for i in popular_words_lst[:MAX_SPEC_WORDS]:\n",
    "    spec_words.add(i[0])\n",
    "print(spec_words)\n",
    "print('popular -- ok')\n",
    "\n",
    "\n",
    "max_features = 50\n",
    "X_test = []\n",
    "groups_test = []\n",
    "cnt = 1\n",
    "for new_group in testgroups_titledata:\n",
    "    docs = testgroups_titledata[new_group]\n",
    "    for k, (doc_id, title) in enumerate(docs):\n",
    "        groups_test.append(new_group)\n",
    "        all_dist = []\n",
    "        # приводим слова к нормальной форме и помещаем их в сет\n",
    "        words = words_list[doc_id].copy()\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j = docs[j]\n",
    "            words_j = words_list[doc_id_j].copy()\n",
    "            all_dist.append(len(words.intersection(words_j)))\n",
    "\n",
    "        if len(all_dist) < max_features:\n",
    "            all_dist.extend([0] * (max_features - len(all_dist)))\n",
    "        X_test.append(sorted(all_dist, reverse=True)[0:max_features])\n",
    "    print(\"{} -- ok\".format(cnt))\n",
    "    cnt += 1\n",
    "X_test = np.array(X_test)\n",
    "groups_test = np.array(groups_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настраиваем классификатор и обучаем его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsRegressor(n_neighbors=32, algorithm='kd_tree', )\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#делаем предсказание для тестовой выборки\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "for i in np.where(y_pred >= 0.4):\n",
    "    y_pred[i] = 1\n",
    "for i in np.where(y_pred < 0.4):\n",
    "    y_pred[i] = 0\n",
    "y_pred = np.array(y_pred, dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#добавляем столбец target\n",
    "test_data['target'] = y_pred\n",
    "\n",
    "#удаляем ненужные столбцы\n",
    "test_data.drop(['group_id', 'doc_id'], axis='columns', inplace=True)\n",
    "\n",
    "#сохраняем наш датафрейм в файл, который загружаем в kaggle\n",
    "test_data.to_csv('result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы с Вами рассмотрели один из вариантов построения признаков в задаче. Однако, это не значит, что другие варианты построения признаков не имеют права на жизнь. Некоторые веб-страницы вообще нельзя присоединить к тематике (спам, например) или, возможно, некоторые группы задают широкую тему (холодильники), а некоторые более узкую (холодильники сделанные в  Советском Союзе в 80х годах). Таким образом, признаки, которыми мы будем описывать пару (группа, веб-страница) можно разделить на:\n",
    "\n",
    "1) Зависят от пары (группа, веб-страница) самые важные признаки\n",
    "\n",
    "2) Зависят от группы\n",
    "\n",
    "3) Зависят от веб-страницы\n",
    "\n",
    "Мы в рассмотрели только самую важную (на наш взгляд) первую группу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Правила конкурса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конкурс проходит на платформе kaggle по адресу, который есть в чате и на портале. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные правила:\n",
    "1. Соревнование длится до конца курса\n",
    "2. Баллы будут проставлены в зависимости от итоговой позици команды в рейтинге\n",
    "3. При непродолении бейзлайна проект для команды не считается засчитанным\n",
    "4. Команда может быть не более 4 человек\n",
    "5. Весь код должен быть закомичен на гитхаб для проверки\n",
    "\n",
    "Чего делать нельзя (карается незащитой проекта):\n",
    "1. Обмениваться кодом между командами вне общего слака\n",
    "2. Использовать ручную разметку\n",
    "\n",
    "Чем можно пользоваться:\n",
    "любыми алгоритмами, любыми дополнительными данными. \n",
    "\n",
    "Любые вопросы задавать в слаке @vikulin_seva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Советы по решению"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перечислим здесь идеи, которые пришли нам самим в голову. Это только наше мнение, ему следовать не обязательно :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Использовать нормализацию текста при подсчете текстовой похожести (см. лекцию 11 или интернет, библиотеку nltk, pymorphy)\n",
    "2. Очевидно, просто число общих слов не идеальная метрика похожести. Как минимум, не учитывает длину, не учитывает, что бывают популярные слова (в, и, на), которые везде встречаются. \n",
    "3. Брать другую информацию из html страницы (url, body, meta...)\n",
    "4. Попробовать лучше понять структуру в рамках одной группы с помощью методов кластеризации и добавить ее выход в признаки (см лекцию 8 или интернет)\n",
    "5. Посмотреть в сторону методов детекции аномалий, которые явным образом в курсе не рассматриваются (см интернет, например, вот https://dyakonov.org/2017/04/19/поиск-аномалий-anomaly-detection/) и их выход добавлять в признаки\n",
    "6. Попробовать разные алгоритмы машинного обучения\n",
    "\n",
    "Интутивно кажется, что в этом конкурсе  важнее построение признаков, чем сам алгоритм, но, возможно, интуиция нас подводит."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cоветы не только на этот конкурс: \n",
    "1. Смотреть на данные, только так можно придумать хорошее решение\n",
    "2. Обычно, самые красивые решения являются самыми простыми (см http://alexanderdyakonov.narod.ru/intro2datamining.pdf http://alexanderdyakonov.narod.ru/lpotdyakonov.pdf). Постарайтесь их найти :)\n",
    "3. Не доверять полностью публичному значению качества на kaggle, ВСЕГДА использовать валидацию\n",
    "4. Пробовать усреднять предсказания разных моделей, это может работать лучше, чем каждая по отдельности. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удачи!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
